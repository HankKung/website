<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hank Chi-Hsi Kung</title>

    <meta name="author" content="Chi-Hsi Hank Kung">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hank Chi-Hsi Kung 
                </p>
                
                <p>
                  I am a visiting researcher at Indiana University Bloomington where I am working with Prof. <a href="https://homes.luddy.indiana.edu/djcran/">David Crandall</a> and Prof. <a href="https://psych.indiana.edu/directory/faculty/smith-linda.html">Linda Smith</a>. My current research focuses on 3D representation learning in both human and machine intelligence, aiming to reverse-engineer the process by which children develop 3D representation learning.
                 </p>
                 <p>Previously, I was a research assistant at <a href="https://www.nycu.edu.tw/nycu/en/index">National Chiao Tung University</a> in Taiwan, where I focused on visual compositional representation and self-supervised video representation learning under the guidance of Prof. <a href="https://sites.google.com/site/yitingchen0524/">Yi-Ting Chen</a> and Dr. <a href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>.</p>
                 <p>
                  I was a research intern at IBM Thomas J. Watson Research Center. I received my M.Sc from National Tsing-Hua University, where I was supervised by Prof. <a href="http://www.cs.nthu.edu.tw/~cherung/">Che-Rung Lee</a>, and B.Sc from National Taipei University. 
                  <!-- I will be visiting CMU and work with Prof. <a href="https://davheld.github.io/">David Held</a> soon! -->
                </p>
                <p><strong>I am actively seeking a Ph.D. position starting in Fall 2025</strong></p>
                <!-- <p>  -->
                <!-- I'm currently looking for a Ph.D position for 2024 Fall! See you in ICCV Paris! -->
                <!-- </p> -->
                <p style="text-align:center">
                  <a href="mailto:hank910140@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com.tw/citations?user=qdyuCMQAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/7ckung">X</a> &nbsp;/&nbsp;
                  <a href="https://github.com/HankKung">Github</a>&nbsp;/&nbsp;
                  <a href="https://drive.google.com/file/d/1cDmjVHeCq53s9GwMM6pBGsg5Avpe6t2j/view">CV</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a><img style="max-width: 100%; height: auto;" alt="profile photo" src="images/hank_milan1.JPG" class="hoverZoomLink"></a>
              </td>

            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research goal is to reverse engineer the human cognition. <strong>Intuitive Physics World Models</strong>, a foundamental componet enabling human to <strong>imagining</strong> the physical interaction and causality can help machines adapt to novel situations. I aim to build Intuitive Physics World Models to facilitate human-like intelligence. Toward this, <strong>learning compositional and augmentable representations</strong> is a crucial component as physical interaction and object properties are compositional concepts and can be "reused" to form new patterns and concepts.

                </p>
                  Moreover, I am fascinated by how humans rapidly learn novel compositions with minimal experience, motivating me to approach <strong>learning compositionality through human learning</strong> by integrating insights from child development and cognitive sciences. 
                <p>

                </p>
              </td>
            </tr>
          </tbody></table>


          <!-- <table style="width:100%;border:20px;border-spacing:0px;border-collapse:collapse;line-height: 1;">
            <tbody>
              <tr>
                <td style="padding:20px;text-align:center;">
                  <h2>News</h2>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;text-align:center;">
                  <p>Aug 2024</p>
                </td>
                <td style="padding:0px;text-align:left;">
                  <p>Give a talk at <a href="https://cvgip2024.csie.ndhu.edu.tw/agenda/forum/">CVGIP 2024</a>!</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;text-align:center;">
                  <p>Apr 2024</p>
                </td>
                <td style="padding:0px;text-align:left;">
                  <p>Co-organizing the <a href="https://sites.google.com/view/road-eccv2024/home?authuser=0">3rd ROAD Workshop & Challenge</a> at <strong>ECCV 2024!</strong></p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;text-align:center;">
                  <p>Mar 2024</p>
                </td>
                <td style="padding:0px;text-align:left;">
                  <p> One paper on <a href="https://hcis-lab.github.io/Action-slot/">Visual Action-centric Representation</a> is accepted at <strong>CVPR 2024!</strong></p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;text-align:center;">
                  <p>Feb 2024</p>
                </td>
                <td style="padding:0px;text-align:left;">
                  <p> One paper on <a href="https://hcis-lab.github.io/RiskBench/">Risk Identification</a> is accepted at <strong>ICRA 2024!</strong></p>
                </td>
              </tr>

            </tbody>
          </table> -->


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Workshop Organizing</h2>
                <p>Organizing <a href="https://sites.google.com/view/reverse-engineeringhumanlearn/home">Reverse-Engineering Human's Learning Mechanisms: Toward Human-Like AI</a> at <strong>NeurIPS 2025</strong> </p>

                <p><a href="https://sites.google.com/view/road-eccv2024/home?authuser=0">3rd ROAD++ Workshop & Challenge of Compositional Representation for Traffic Activities</a> at <strong>ECCV 2024</strong></p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>

            <td style="padding:20px;width:40%;vertical-align:middle;height: 30px;">
              <h2>Publications</h2>
            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/view_selection.png" width=100% height=100%>
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Learning to Look Like Humans: Modeling Planar View Bias with Inductive Priors in Reinforcement Learning</span>
              <br>
              <br>
              <strong>Chi-Hsi Kung</strong>, 
              <a>Frangil Ramirez</a>, 
              <a>Juhyung Ha</a>,
              <a href="https://sites.google.com/site/yitingchen0524">Yi-Ting Chen</a>,
              <a href="https://homes.luddy.indiana.edu/djcran/">David Crandall</a>,
              <a href="https://psych.indiana.edu/directory/faculty/smith-linda.html">Linda Smith</a>
              <br>
              <br>
              <a>Paper coming soon</a>
              /
              <a>Code coming soon</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/counterfactual.gif" width=100% height=100%>
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning</span>
              <br>
              <br>
              <strong>Chi-Hsi Kung</strong>*, 
              <a>Frangil Ramirez*</a>, 
              <a>Juhyung Ha</a>,
              <a href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>, 
              <a href="https://sites.google.com/site/yitingchen0524">Yi-Ting Chen</a>,
              <a href="https://homes.luddy.indiana.edu/djcran/">David Crandall</a>
               (* Equal Contribution)</a>
              <br>
              <em>Under review</em>
              <br>
              <a href="https://arxiv.org/abs/2503.21055">arxiv</a>

              /
              <a>Code coming soon</a>
              <p>
              Procedure-aware video representation learning with state changes and their counterfactuals.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/ATARS.png" width=100% height=100%>
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <a href="https://hcis-lab.github.io/Action-slot/"> -->
                <span class="papertitle">ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal Segmentation Dataset</span>
              <!-- </a> -->
              <br>
              <br>
              <a>Zihao Chen</a>, 
              <a>Hsuanyu Wu</a>, 
              <strong>Chi-Hsi Kung</strong>*, 
              <a href="https://sites.google.com/site/yitingchen0524">Yi-Ting Chen*</a>,
              <a href="https://www.cs.nccu.edu.tw/~ytpeng/">Yan-Tsung Peng*</a>
               (* Equal Advising)</a>
              <br>
              <em>Under review</em>
              <br>
              <!-- <a href="https://hcis-lab.github.io/Action-slot/">project page</a>
              / -->
              <a href="https://arxiv.org/abs/2503.18553">arxiv</a>
              <!-- / -->
              <!-- <a href="https://arxiv.org/abs/2311.17948">arxiv</a> -->
              /
              <a href="https://github.com/magecliff96/ATARS/">Code</a>
              /
              <!-- <a href="https://nycu1-my.sharepoint.com/:f:/g/personal/ychen_m365_nycu_edu_tw/EnRg1zT7CeZGg3Ju2TIP1j8B0NB0fCpYsjGQBc0Tcf2H6w?e=FGJvTc">TACO dataset</a> -->
              
              <p>
              The first drone-view traffic dataset for compositional action recognition.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/scenario_generation.png" width=100% height=100%>
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <a href="https://hcis-lab.github.io/Action-slot/"> -->
                <span class="papertitle">Controllable Scenario-based Collision Generation for Safety Assessment</span>
              <!-- </a> -->
              <br>
              <br>
              <a>Pin-Lun Chen</a>, 
              <strong>Chi-Hsi Kung</strong>, 
              <a href="https://scholar.google.com/citations?user=LLoPMDMAAAAJ&hl=en">Che-Han Chang</a>,
              <a href="https://walonchiu.github.io/">Wei-Chen Chiu</a>,
              <a href="https://sites.google.com/site/yitingchen0524">Yi-Ting Chen</a>
              <br>
              <em>Under review</em>
              <br>
              <!-- <a href="https://hcis-lab.github.io/Action-slot/">project page</a>
              / -->
              <a>Paper coming soon</a>
              <!-- / -->
              <!-- <a href="https://arxiv.org/abs/2311.17948">arxiv</a> -->
              /
              <a>Code coming soon</a>
              /
              <!-- <a href="https://nycu1-my.sharepoint.com/:f:/g/personal/ychen_m365_nycu_edu_tw/EnRg1zT7CeZGg3Ju2TIP1j8B0NB0fCpYsjGQBc0Tcf2H6w?e=FGJvTc">TACO dataset</a> -->
              
<!--               <p>
              The first drone-view traffic dataset for compositional action recognition.
              </p> -->
            </td>
          </tr>


            <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/action-slot.gif" width=100% height=100%>
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <a href="https://hcis-lab.github.io/Action-slot/"> -->
                <span class="papertitle">Action-slot: Visual Action-centric Representations for Atomic Activity Recognition in Traffic Scenes</span>
              <!-- </a> -->
              <br>
              <br>
              <strong>Chi-Hsi Kung</strong>, 
              <a>Shu-Wei Lu</a>, 
              <a href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>, 
              <a href="https://sites.google.com/site/yitingchen0524">Yi-Ting Chen</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://hcis-lab.github.io/Action-slot/">project page</a>
              /
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kung_Action-slot_Visual_Action-centric_Representations_for_Multi-label_Atomic_Activity_Recognition_in_CVPR_2024_paper.html">paper</a>
              /
              <a href="https://arxiv.org/abs/2311.17948">arxiv</a>
              /
              <a href="https://github.com/HCIS-Lab/Action-slot/tree/main">code</a>
              /
              <a href="https://nycu1-my.sharepoint.com/:f:/g/personal/ychen_m365_nycu_edu_tw/EnRg1zT7CeZGg3Ju2TIP1j8B0NB0fCpYsjGQBc0Tcf2H6w?e=FGJvTc">TACO dataset</a>
              
              <p>
              We use Action-slot to represent atomic activities. The learned attention can discover and localize atomic activities with only weak video labels and without using any perception module (e.g., object detector).
              </p>
            </td>
      </tr>

      <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/riskbench.gif" width=100% height=100%>
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <a href="https://hcis-lab.github.io/RiskBench/"> -->
                <span class="papertitle">RiskBench: A Scenario-based Benchmark for Risk Identification</span>
              <!-- </a> -->
              <br>
              <br>
              <strong>Chi-Hsi Kung</strong>, 
              <a>Chieh-Chi Yang</a>, 
              <a>Pang-Yuan Pao</a>, 
              <a>Shu-Wei Lu</a>, 
              <a>Pin-Lun Chen</a>, 
              <a>Hsin-Cheng Lu</a>, 
              <a href="https://sites.google.com/site/yitingchen0524">Yi-Ting Chen</a>
              <br>
              <em>ICRA</em>, 2024
              <br>
              <a href="https://hcis-lab.github.io/RiskBench/">project page</a>
              /
              <a href="https://www.youtube.com/watch?v=9VQV7TRmwl4">video</a>
              /
              <a href="https://arxiv.org/abs/2312.01659">paper</a>
              /
              <a href="https://github.com/HCIS-Lab/RiskBench">code</a>
              /
              <a href="https://nycu1-my.sharepoint.com/personal/ychen_m365_nycu_edu_tw/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fychen%5Fm365%5Fnycu%5Fedu%5Ftw%2FDocuments%2FRiskBench&ga=1">dataset</a>
              <p>
              The FIRST benchmark that enables evaluation of various types of risk identification algorithms, namely, rule-based, trajectoy-prediction-based, collision prediction, and behavior-change-based. We also assess the influence of risk identification to the downstream driving task.
              </p>
            </td>
      </tr>
      <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
  <!--               <div class="one">
                <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/bakedsdf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/bakedsdf_before.jpg' width="160">
              </div> -->
                 <!-- <div class="content has-text-justified">
                 <img src="images/ADD.png">
                 </d -->
                 <img src="images/ADD.png" width=100% height=100%>
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <a href="https://ieeexplore.ieee.org/document/9636650"> -->
                <span class="papertitle">ADD: A Fine-grained Dynamic Inference Architecture for Semantic Image Segmentation</span>
              <!-- </a> -->
              <br>
              <br>
              <strong>Chi-Hsi Kung</strong> and
              <a href="http://www.cs.nthu.edu.tw/~cherung/">Che-Rung Lee</a>
              <br>
              <em>IROS</em>, 2021 & <a href="https://mrvc-2021.net/">ACML 2021 MRVC workshop</a>
              <br>
              <a href="https://ieeexplore.ieee.org/document/9636650">paper</a>
              /
              <a href="https://github.com/HankKung/Auto-Dynamic-DeepLab">code</a>
              <p>
              We use Neural Architecture Search (NAS) to find an optimal structure for dynamic inference on semantic segmentation.
              </p>
            </td>
      </tr>
          </tbody>
        </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Conference Reviewer</h2>
                
                <p>IEEE Conference on Computer Vision and Pattern Recognition <strong>CVPR</strong> 2023-2025</p>
                <p>The International Conference on Machine Learning <strong>ICML</strong> 2025</p>
                <p>International Conference on Computer Vision <strong>ICCV</strong> 2025</p>
                <p>Advances in Neural Information Processing Systems <strong>NeurIPS</strong> 2024</p>
                <p>IEEE International Conference on Development and Learning <strong>ICDL</strong> 2024</p>
                <p>IEEE/RSJ International Conference on Intelligent Robots and Systems <strong>IROS</strong> 2025</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Invited Talk</h2>
                <p><a href="https://cvgip2024.csie.ndhu.edu.tw/agenda/forum/">CVGIP 2024</a></p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>


